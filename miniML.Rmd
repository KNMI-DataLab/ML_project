---
title: "ML_10min"
author: "Eva Kleingeld"
date: "August 4, 2016"
output: pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


rm(list=ls())
  
#install required packages here

#install.packages("Metrics")
#install.packages("rattle")
#install.packages("caret", dependencies = c("Imports", "Depends", "Suggests"))
#install.packages("rpart.plot")
#install.packages("DMwR")
install.packages("RSNNS")
```


The following code reads in the 10 min subset and environmental data. 
All "suspect" data is removed.
The two datasets are then merged.
The merged dataset is split in a test and training set. 

```{r}
## Read in subset & environmental data

#For 5 min: load("/usr/people/kleingel/R/Data_subsets/Data_5min.Rda")
#Data_5min<-Data_5min[Data_5min$QUALITY=="valid", ]

load("/usr/people/kleingel/Projects/MLProject/Data_10min.Rda")
Data_10min <- Data_10min[Data_10min$QUALITY == "valid", ]

## Drop the quality column
Data_10min <- Data_10min[ ,-7]

load("/usr/people/kleingel/Projects/MLProject/Env_Data.Rda")

## Merge subset and environmental data
data_GMS<-merge(Data_10min,Env_Data_4,by.x=c("LOCATION","SENSOR"),by.y=c("MISD","SENSOR"))

## Split data_GMS into a training and test set

Time_1 <- as.POSIXct("2016-06-28 00:05:00", format = "%Y-%m-%d %H:%M:%S", tz = "GMT")
Time_2 <- as.POSIXct("2016-06-28 00:10:00", format = "%Y-%m-%d %H:%M:%S", tz = "GMT")  

Train_data <- subset(data_GMS, data_GMS$TIMESTAMP == Time_1) 
Test_data <- subset(data_GMS, data_GMS$TIMESTAMP == Time_2)

```

Now, we start building models based on different ML algorithms. 

First, we build a linear model 

```{r}
library(caret)

n<-colnames(data_GMS)[7:(length(colnames(data_GMS)))]
f <- as.formula(paste("TEMP ~", paste(n[!n %in% "TEMP"], collapse = " + ")))

LinearModel <- train(form = f, data = Train_data, method = "lm")
LinearModel2 <- train(x = Train_data[7:(length(colnames(data_GMS)))], y = Train_data$TEMP, method = "lm")

# Get the lm summary
# The summary indicates that five variables are perfectly collinear (Coefficients: 5 not defined because of singularities)
summary(LinearModel)

# Calculate RMSE 
Linear_Residuals <- residuals(LinearModel)
sqrt(mean((Linear_Residuals)^2))

# Caret RMSE
LinearModel$results

# Predict with the lm model
Linear_Predict <- extractPrediction(list(LinearModel), testX = Test_data[7:(length(colnames(data_GMS)))], testY = Test_data$TEMP)

# Plot observed vs predicted
plotObsVsPred(Linear_Predict)
ggplot(data = Linear_Predict) + geom_point(aes(x = obs, y = pred)) 


```




Here, a neuralnet neural network is built. 
```{r}

# Build a neural network with neuralnet (from neuralnet package)
library(neuralnet)

# First build a neural network without TL and TD
# If you include TL and TD you may have to remove some stations because these variables can be NA -> then NN won't run

n<-colnames(data_GMS)[7:(length(colnames(data_GMS)))]
f <- as.formula(paste("TEMP ~", paste(n[!n %in% "TEMP"], collapse = " + ")))
nn<-neuralnet(f,data=Train_data,hidden=c(5,3),linear.output=T)
plot(nn)

pr.nn <- compute(nn,Test_data[,7:length(colnames(Train_data))]) # select only relevant variables

# Calculate the MSE
library(Metrics)
library(ggplot2)

mse(as.vector(Train_data$TEMP), as.vector(pr.nn$net.result))
rmse(as.vector(Train_data$TEMP), as.vector(pr.nn$net.result))

## plot the actual values v.s. the predicted values
ggplot() + geom_point(aes(x = as.vector(Train_data$TEMP), y = as.vector(pr.nn$net.result))) +
           labs(x = "Measured temperature", y = "Predicted temperature")

```

Here a multilayer perceptron is built (: a neural network - like algorithm)

```{r}
library(RSNNS)


```





Build a decision tree
```{r}
# Building a tree with the caret package
library(caret)
library(rpart)
library(rattle)

# Build a tree model with the train data
# There are multiple ways to do this: see below
# Warning message rpart, gone but still bugs when using rpart2
TreeModel <- train(form = f, data = Train_data, method = "rpart")
TreeModel2 <- train(x  = Train_data[7:(length(colnames(data_GMS)))], y = Train_data$TEMP, method = "rpart")
compiler::setCompilerOptions(optimize = 1)


# Plot the decision tree you have built (:TreeModel$finalModel)
fancyRpartPlot(TreeModel$finalModel)
fancyRpartPlot(TreeModel2$finalModel)

# In order to plot a basic tree
plot(TreeModel$finalModel, uniform = TRUE)
text(TreeModel$finalModel, use.n = TRUE, all = TRUE)

# To test variable importance you can use
TreeModel2_Imp <- varImp(TreeModel2)
plot(varImp(TreeModel2))


# Predict with test set based on TreeModel
# The predict() function predicts TW values (:numeric)
# You can write both TreeModel$finalModel and TreeModel
Tree_Predict <- predict(TreeModel2, newdata = Test_data[7:(length(colnames(data_GMS)))])
plot(Tree_Predict)

# The extractPrediction() function gives you more information
# Put your model in a list, because otherwise you can get an error
Tree_Predict2 <- extractPrediction(list(TreeModel), testX = Test_data[7:(length(colnames(data_GMS)))], testY = Test_data$TEMP)

# Plot observed vs. predicted 
plotObsVsPred(Tree_Predict2)

# Plot manually (ggplot2) 
ggplot() + geom_point(aes(x = Tree_Predict2$obs, y = Tree_Predict2$pred))

```

Build a Random Forest (RF)
``` {r}
library(randomForest)

# Buil a Random Forest (RF) with the training data set
# The RF is much slower to build than the decision tree
# Can be useful to set importance to TRUE, does increase computation time
# You can train the RF model in two different ways.
# The first requires you to specify a formula and a dataframe (comp time: 220.6 seconds)
# The second requires you to specify the predictor values and response variable (comp time: 219.5 seconds)
system.time(RFModel <- train(form = f, data = Train_data, method = "parRF", importance = TRUE))
compiler::setCompilerOptions(optimize = 1)

system.time(RFModel2 <- train(x = Train_data[7:(length(colnames(data_GMS)))], y = Train_data$TEMP, method = "parRF", importance = TRUE))
compiler::setCompilerOptions(optimize = 1)

# Plot the RF model you have built: How many trees until the error decreases? 
plot(RFModel$finalModel,plotType="level")

# Find a way of beautifully plotting a RF

# Importance of input variables/features for RF:
# Importance measure = 1 = mean decrease in accuracy = IncMSE
# This can only be calculated if importance = TRUE in train() (part of RandomForest package)
Importance_RF_1 <- importance(RFModel$finalModel, type = 1)
dotplot(Importance_RF_1)

# Importance measure = 2 = decrease in node impurity
Importance_RF_2 <- importance(RFModel$finalModel, type = 2)
dotplot(Importance_RF_2)

# With varImp
Importance_RF_3 <- varImp(RFModel, scale = FALSE)
plot(Importance_RF_3)

# Make a partial plot
# A partial plot does not make sense for dummy variables. Relevant columns are 7 until 10. Partial plots are only made for relevant columns. 
# Train_data argument in partialPlot is set to a subset of the data because partialPlot cannot handle NA. But because f excludes columns up until Train_data[ ,7] this should not pose a problem
# The for loop uses seq_along() and vector[i] structure because partialPlot throws an error otherwise

Rel_vars <- colnames(Train_data[ ,7:10])

par(mfrow = c(1, length(Rel_vars)))

for (i in seq_along(Rel_vars)){
  print(i)
  partialPlot(RFModel$finalModel, pred.data = Train_data[7:(length(colnames(data_GMS)))], x.var = Rel_vars[i], xlab = Rel_vars[i], ylab = "Model predicted temperature", main = paste("Partial plot of", Rel_vars[i]))
  
}

dev.off()

# Predict with your RF
# I suspect that the first RF_predict won't run because the input for the extractPrediction function differs from the input that the train function was given. Therefore it may be a good idea to run the train function with the RFModel2 settings. 
# However, TreeModel does not seem to have this issue?
RF_Predict <-  extractPrediction(list(RFModel), testX = Test_data[7:(length(colnames(data_GMS)))], testY = Test_data$TEMP)
RF_Predict2 <-  extractPrediction(list(RFModel2), testX = Test_data[7:(length(colnames(data_GMS)))], testY = Test_data$TEMP)

# Plot observations versus predictions
# Why does plotObsVsPred have two the same graphs?
plotObsVsPred(RF_Predict2)
ggplot(data = RF_Predict2) +geom_point(aes(x = pred, y = obs))



```




